{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e7e42f-88de-40a4-9411-63206a7a1836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple, TypedDict, Union\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "from pystac_client import Client\n",
    "from rasterio.mask import mask\n",
    "from rasterio.session import AWSSession\n",
    "from shapely.geometry import shape\n",
    "from stackstac import stack\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.tools import tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "NL_classification_values = {\n",
    "    2: \"natural forests\",\n",
    "    3: \"natural short vegetation\",\n",
    "    4: \"natural water\",\n",
    "    5: \"mangroves\",\n",
    "    6: \"bare\",\n",
    "    7: \"snow\",\n",
    "    8: \"wet natural forests\",\n",
    "    9: \"natural peat forests\",\n",
    "    10: \"wet natural short vegetation\",\n",
    "    11: \"natural peat short vegetation\",\n",
    "    12: \"crop\",\n",
    "    13: \"built\",\n",
    "    14: \"non-natural tree cover\",\n",
    "    15: \"non-natural short vegetation\",\n",
    "    16: \"non-natural water\",\n",
    "    17: \"wet non-natural tree cover\",\n",
    "    18: \"non-natural peat tree cover\",\n",
    "    19: \"wet non-natural short vegetation\",\n",
    "    20: \"non-natural peat short vegetation\",\n",
    "    21: \"non-natural bare\"\n",
    "}\n",
    "\n",
    "DEFAULT_COG_PATH = \"s3://gfw-data-lake/umd_glad_dist_alerts/v20250329/raster/epsg-4326/cog/default.tif\"\n",
    "INTENSITY_COG_PATH = \"s3://gfw-data-lake/umd_glad_dist_alerts/v20250329/raster/epsg-4326/cog/intensity.tif\"\n",
    "STAC_API_URL = \"https://eoapi.zeno-staging.ds.io/stac\"\n",
    "AWS_PROFILE = \"zeno_internal_sso\"\n",
    "REFERENCE_DATE = datetime(2015, 1, 1)\n",
    "\n",
    "def convert_date_range_to_days(date_range):\n",
    "    start_date = datetime.strptime(str(date_range[0])[:10], \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(str(date_range[1])[:10], \"%Y-%m-%d\")\n",
    "    return (start_date - REFERENCE_DATE).days, (end_date - REFERENCE_DATE).days, start_date, end_date\n",
    "\n",
    "def load_alert_data(geometry):\n",
    "    session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "    with rasterio.Env(AWSSession(session), AWS_REQUEST_PAYER=\"requester\"):\n",
    "        with rasterio.open(DEFAULT_COG_PATH) as src1:\n",
    "            default_data, _ = mask(src1, geometry, crop=True)\n",
    "        with rasterio.open(INTENSITY_COG_PATH) as src2:\n",
    "            intensity_data, _ = mask(src2, geometry, crop=True)\n",
    "    return default_data[0], intensity_data[0]\n",
    "\n",
    "def load_natural_lands_mosaic(aoi_geom, start_date, end_date):\n",
    "    stac = Client.open(STAC_API_URL)\n",
    "    search = stac.search(\n",
    "        collections=[\"natural-lands-map-v1-1\"],\n",
    "        intersects=aoi_geom,\n",
    "        datetime=f\"{start_date.date().isoformat()}/{end_date.date().isoformat()}\",\n",
    "        max_items=50,\n",
    "    )\n",
    "    items = list(search.get_items())\n",
    "    if not items:\n",
    "        raise ValueError(\"No Natural Lands items found for AOI.\")\n",
    "\n",
    "    print(f\"Found {len(items)} items\")\n",
    "\n",
    "    da = stack(\n",
    "        items,\n",
    "        bounds_latlon=aoi_geom.bounds,\n",
    "        snap_bounds=True,\n",
    "        epsg=4326\n",
    "    )\n",
    "    \n",
    "    # Apply chunking\n",
    "    da = da.chunk({\"x\": 1024, \"y\": 1024})\n",
    "\n",
    "\n",
    "    # Collapse time dimension if multiple timestamps\n",
    "    if \"time\" in da.dims:\n",
    "        da = da.astype(\"int16\").max(\"time\")\n",
    "\n",
    "    return da.squeeze()\n",
    "\n",
    "@tool\n",
    "def parse_user_message(state: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Parses a user message into structured input for filtering pixels.\n",
    "    Expects keys: 'message' in state.\n",
    "    Returns: a dictionary with 'aoi', 'date_range', and 'thresholds'.\n",
    "    \"\"\"\n",
    "    message = state.get(\"message\", \"\")\n",
    "    geojson = state.get(\"aoi\", {})\n",
    "\n",
    "    date_match = re.search(r'date:\\s*(\\d{4}-\\d{2}-\\d{2})\\s*to\\s*(\\d{4}-\\d{2}-\\d{2})', message)\n",
    "    confidence_match = re.search(r'confidence:\\s*(\\d+)', message)\n",
    "    intensity_match = re.search(r'intensity:\\s*([0-9.]+)', message)\n",
    "\n",
    "    start_date = date_match.group(1) if date_match else \"2015-01-01\"\n",
    "    end_date = date_match.group(2) if date_match else datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    thresholds = {\n",
    "        \"confidence\": int(confidence_match.group(1)) if confidence_match else 0,\n",
    "        \"intensity\": float(intensity_match.group(1)) if intensity_match else 0.0\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"message\": message,      # keep original message for graph continuity\n",
    "        \"aoi\": geojson,\n",
    "        \"date_range\": (start_date, end_date),\n",
    "        \"thresholds\": thresholds\n",
    "    }\n",
    "\n",
    "#@tool\n",
    "def filter_and_count_pixels(state: Dict) -> Dict:\n",
    "    \"\"\"Filters pixels from COGs using confidence, intensity, and date thresholds, then counts land cover class occurrences.\"\"\"\n",
    "    parsed = state[\"parsed_params\"]\n",
    "\n",
    "    print(\"Parsed parameters:\", parsed)\n",
    "\n",
    "    start_days, end_days, start_date, end_date = convert_date_range_to_days(\n",
    "        (parsed[\"start_date\"], parsed[\"end_date\"])\n",
    "    )\n",
    "    geometry = [shape(parsed[\"aoi\"][\"geometry\"])]\n",
    "\n",
    "    # Load and process data\n",
    "    encoded, intensities = load_alert_data(geometry)\n",
    "    confidence = encoded // 10000\n",
    "    days_since_2015 = encoded % 10000\n",
    "\n",
    "    land_cover = load_natural_lands_mosaic(shape(parsed[\"aoi\"][\"geometry\"]), start_date, end_date)\n",
    "    land_cover = land_cover.squeeze()\n",
    "\n",
    "    if land_cover.shape != confidence.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {land_cover.shape} vs {confidence.shape}\")\n",
    "\n",
    "    max_days = (datetime.today() - REFERENCE_DATE).days\n",
    "    days_since_2015[days_since_2015 == 9999] = days_since_2015[days_since_2015 < 9999].max()\n",
    "\n",
    "    valid_mask = (\n",
    "        (confidence >= parsed[\"confidence_threshold\"]) &\n",
    "        (days_since_2015 >= start_days) &\n",
    "        (days_since_2015 <= end_days) &\n",
    "        (days_since_2015 < max_days) &\n",
    "        (intensities >= parsed[\"intensity_threshold\"])\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "    valid_lc = land_cover.values[valid_mask]\n",
    "    unique_classes, counts = np.unique(valid_lc, return_counts=True)\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        results[int(cls)] = {\n",
    "            \"class\": NL_classification_values.get(int(cls), \"Unknown\"),\n",
    "            \"count\": int(count)\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"summary_json\": {\n",
    "            \"start_date\": parsed[\"start_date\"],\n",
    "            \"end_date\": parsed[\"end_date\"],\n",
    "            \"confidence_threshold\": parsed[\"confidence_threshold\"],\n",
    "            \"intensity_threshold\": parsed[\"intensity_threshold\"],\n",
    "            \"results\": results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class ParsedParams(BaseModel):\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    confidence_threshold: float\n",
    "    intensity_threshold: int\n",
    "    aoi: Union[str, Dict]  # named region or GeoJSON\n",
    "\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    input: str\n",
    "    parsed_params: dict\n",
    "    results: dict\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class ParseParamsNode:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"input_text\"],\n",
    "            template=\"\"\"\n",
    "Extract the following parameters from the input text:\n",
    "- start_date (YYYY-MM-DD)\n",
    "- end_date (YYYY-MM-DD)\n",
    "- confidence_threshold (float)\n",
    "- intensity_threshold (int)\n",
    "- aoi (string or GeoJSON)\n",
    "\n",
    "Input:\n",
    "{input_text}\n",
    "\n",
    "Return a JSON object only with these keys and values.\n",
    "Please output ONLY valid JSON with all braces closed. Do not add any extra text or explanations.\n",
    "Make sure the JSON is valid and complete.\n",
    "\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def run(self, input_text: str) -> ParsedParams:\n",
    "        response = self.chain.run(input_text=input_text)\n",
    "\n",
    "        # Parse JSON output\n",
    "        try:\n",
    "            params_dict = json.loads(response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Failed to parse JSON from LLM output: {response}\") from e\n",
    "\n",
    "        # Validate and return Pydantic model\n",
    "        return ParsedParams(**params_dict)\n",
    "\n",
    "\n",
    "\n",
    "parser_node = ParseParamsNode()\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant summarizing geospatial alert statistics.\"),\n",
    "    (\"human\", \"Here are the results:\\n{summary_json}\")\n",
    "])\n",
    "summarize_chain = summary_prompt | llm | RunnableLambda(lambda msg: {\"summary\": msg.content})\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"parse_message\", RunnableLambda(lambda state: {\n",
    "    \"parsed_params\": parser_node.run(state[\"input\"]).dict()\n",
    "}))\n",
    "\n",
    "def filter_and_count_node(state):\n",
    "    result = filter_and_count_pixels(state)\n",
    "    return {\n",
    "        **state,\n",
    "        \"results\": result[\"summary_json\"][\"results\"]\n",
    "    }\n",
    "\n",
    "builder.add_node(\"filter_and_count\", RunnableLambda(filter_and_count_node))\n",
    "\n",
    "builder.add_node(\"summarize_results\", RunnableLambda(\n",
    "    lambda state: (\n",
    "        print(\"SUMMARY:\", summarize_chain.invoke({\"summary_json\": state[\"results\"]})[\"summary\"]),\n",
    "        {\"summary\": summarize_chain.invoke({\"summary_json\": state[\"results\"]})[\"summary\"]}\n",
    "    )[-1]\n",
    "))\n",
    "\n",
    "builder.set_entry_point(\"parse_message\")\n",
    "builder.add_edge(\"parse_message\", \"filter_and_count\")\n",
    "builder.add_edge(\"filter_and_count\", \"summarize_results\")\n",
    "builder.add_edge(\"summarize_results\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1265cf3f-afbd-41b9-9ac6-8f72e22b9492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3l/ltxhyhhn7jn3xwtvypy7f9ym0000gn/T/ipykernel_26676/1603914818.py:286: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"parsed_params\": parser_node.run(state[\"input\"]).dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed parameters: {'start_date': '2019-01-01', 'end_date': '2020-12-31', 'confidence_threshold': 3.0, 'intensity_threshold': 50, 'aoi': {'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-75.0, -3.0], [-70.0, -3.0], [-70.0, -6.0], [-75.0, -6.0], [-75.0, -3.0]]]}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilliannethomas/Desktop/work/wri/project-zeno/.venv/lib/python3.11/site-packages/pystac_client/item_search.py:888: FutureWarning: get_items() is deprecated, use items() instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "SUMMARY: Here are the summarized geospatial alert statistics:\n",
      "\n",
      "- Natural forests: 659 alerts\n",
      "- Natural water: 555 alerts\n",
      "- Bare land: 336 alerts\n",
      "- Wet natural forests: 271 alerts\n",
      "- Natural peat forests: 213 alerts\n",
      "- Wet natural short vegetation: 115 alerts\n",
      "- Natural peat short vegetation: 188 alerts\n",
      "- Crop fields: 3096 alerts\n",
      "- Built-up areas: 20 alerts\n",
      "- Non-natural short vegetation: 9 alerts\n"
     ]
    }
   ],
   "source": [
    "geojson_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"Feature\",\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [-75.0, -3.0],\n",
    "        [-70.0, -3.0],\n",
    "        [-70.0, -6.0],\n",
    "        [-75.0, -6.0],\n",
    "        [-75.0, -3.0]\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "initial_state = {\n",
    "    \"input\": (\n",
    "        \"Find alerts from January 1, 2019 to December 31, 2020, with confidence of 3 and intensity above 50 \"\n",
    "        \"in this AOI (in GeoJSON format): ```json\\n\" +\n",
    "        geojson_str.strip() +\n",
    "        \"\\n```\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bca12-a4c1-4563-a620-3384a3dcbcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
